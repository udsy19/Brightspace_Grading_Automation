{
  "student_name": "Alexander Emenhiser",
  "org_defined_id": "035713425",
  "username": "aemenhi",
  "video_url": "https://youtu.be/DPEfP-TDbKc",
  "transcript": "Hello, my name is Alexander Emenhiser and this is my video for the session 12 assignment. So, today is currently Sunday, December 7th, and I am in the 12:30 section of management 310. In today's video, I'm going to be delivering a full valuation of Panera Bread for prospective KKR takeover. Uh in this I'm going to be com uh combining DCF trading comps and precedent transactions to produce an informed bid range while uh constraining financial buyer disciplined with strategic buyer synergies. We have several variables as we can see from our deal snapshot here that I will be accounting for later in our session. And our goal is to use the data from this snapshot to create an executive ready recommendation that states our valuation range, winning bid strategy, and rationale for choosing financial versus strategic positioning. All of that kind of here in the overview of the assignment. There are several financial analysis requirements. Uh as we can see here, um which we will all be covering uh during this video. We'll be answering a lot of these questions through our Python code which I've used the assistance of AI for. Uh the AI helps account for all of the grunt work or sort of hard coding. uh while I confirm with my own judgment uh that the data and the output is correct and if not then I adjust accordingly. So like all previous sections I will be using the driver framework of learning to help sort of guide my analysis.\n\nSo let's begin our evaluation of an error bread. So as we know from all of our previous sections as this is the last one um the first step of our driver framework is to of course define and discover. And I've already done a little bit of that but let's go into it a little bit more. So in the define and discover stage, my goal is to establish exactly what valuation problem we are going to be solving and what information we need to solve it. For this assignment, we're valuing Panera Bread from the prospective KKR as a potential private equity buyer. That means the output of our work is not just a theoretical valuation. It is a transactionoriented assessment that must ultimately inform a bid strategy. To get there, we need to produce a complete valuation using multiple methods. Understand how uh each method behaves under different assumptions and then determine what KKR should be willing to pay compared to what a strategic buyer uh might pay. To define the problem clearly, we start with the deal snapshot. And this is where I'm going to talk about it a little bit more. So, Panera currently generates about 900 million of the EBITDA at a 15% margin, 400 million in net income, and operates 2100 stores with digital sales already representing half of the revenue. The company also has several growth levers including opening 500 new stores over the next 5 years and expanding digital mix to 70%, developing dinner and catering and beginning international roll out. On the market side, we know that the industry uh P divided by E averages 22 times. President transactions fall between 12x and 15% 15X EBITDA and highly scaled peers like Chipotle and McDonald's trade at premium multiples. The industry WACC is roughly 9% and the management expects a 10% rollover which will also factor into how a financial buyer structures the deal. In the discover portion, we gather the specific inputs needed to build a complete model. The revenue assumptions, the uh margin expectations, capex needs, store growth strategies, WACC, terminal growth, pure comparable sets. These inputs will feed directly into the Python code that runs on our multi-senario DCF trading comps and president transactions. Importantly, this stage also clarifies what must be included for the analysis to be credible, which is a base upside and downside scenario. a terminal value justified by industry norms, a synergy model for strategic buyers and sensitivity tests for WACC, terminal growth, and synergy capture. All of these elements ensure that our valuation is disciplined, transparent, and robust enough to defend in an investment committee setting. By completing the define and discover stage, we now know what question we're answering, what data we're relying on, and what outputs we must generate. With that foundation set, we now move to our second stage of the driver framework of learning, the represent stage, where we will map out exactly how the code will be structured and how the different components of our valuation will connect. So now that we've defined the valuation problem and gathered key data from our deal snapshot, we can now move into the represent stage. This is where I outline the structure of the analysis, map the logic behind the model, and show how each component of the valuation connects before we actually begin coding. Represent is essentially the blueprint of the project. So it's the bridge between the conceptual understanding and the technical implement implementation. So in this stage I began by translating Panera's business drivers into programmable steps. The model needs to reject Panera's performance across all three scenarios base upside and downside. So I outlined how revenue will grow, how store expansion contributes to the top line, how margins evolve, and how these pieces ultimately flow down to free cash flow. I also map out how the WACC terminal growth rate and capital structure assumptions will be incorporated into the DCF. This gives me a clear picture of how the 5-year operating projection connects directly to enterprise values. Next, I lay out the logic for trading comps and precedent transactions. Here the represent stage helps me define which peers will be included, which multiples will be used and how those multiples will be translated into valuation ranges for Panera. Represent also allows me to clarify how I'm going to adjust the model for a strategic buyer scenario. specifically where synergies get layered in, how they affect EBI, TDA, and how they influence a potential premium above what a financial buyer like KKR might pay. This stage is also where I design the sensitivity framework. I plan out how the code will allow for quick adjustments into the WACCC, terminal growth, synergy capture, and store expansion. Representing this logic up front ensures that the sensitivity outputs in the valuation football field are clean, organized and easy to prevent visually easy to present visually during the walkthrough. Finally, I map out the entire how the entire output will be structured. So DCF results first, comps next, precedence after that, followed by a triangulated valuation summary and a recommended bid range. By representing the flow of the analysis before coding anything, I ensure the full valuation is consistent, transparent, and aligned with the driver framework. With this blueprint in place, I can now confidently move into the implement stage where we will bring this structure to life in Python. And that is where we're going to go next. We're going to talk about our code. So in the implement stage, I am going to walk you through our Python script that turns our define and discover plan into an executable valuation. The code is structured as a sequence of small well-n named functions. So every piece of logic is modular, testable, and easy to narrate during our video. Here at the top of the file, we import numpy pandas and mattplot lib and declare two tiny formatting helpers FMT and a dollar round a dollar for dollar rounding and PCT for percentages which we keep all on screen numbers consistent and presentation ready. The first substantive block is set assumptions. So this function contains the devil anchors from our snapshot. So this includes the EBIT TDA, margin, net income, stores, digital mix, and all of the editable parameters that the model uses which includes uh revenue per store, store opening schedule, capex and depreciation as a percent of revenue, NWC, tax rate, base WACC, terminal growth, net debt, peer multiples, and the default synergy assumptions and capture sketch. schedule in the video. Uh we have our function here and uh we have some of the stuff as we see here. So uh the next is our build scenario. So now that we have all of our assumptions here already in place, uh our built scenarios defines three scenario dictionaries of base, upside, and downside that the rest of our model consumes. So we see the three right here. Um each scenario is just a small set of overrides. So same store grow, sales growth, store schedule, margin path, WACC, and terminal growth. And this pattern makes it trivial to add on to other cases or run sensitivity sweeps. So the upside scenario here is to justify more aggressive store schedule and synergy component inputs. The projection and operating model uh functions as a functions loops yearbyear to produce the 5-year operating table for stores new stores revenue EBIT TDA margin ramp EBIT TDA depreciation EBIT NOAD capex and W NWC and finally free cash flow The FCF formula uh is NOPAD plus depreciation minus capex minus NWC. And our margin ramp is a linear interpolation between uh the current margin and the scenario targeted over margin path year. So now we go to our next section which is valuation and for valuation we use discount cash flows here. So this function discounts yearly FCF by scenario by the scenario WACC computes a terminal FCF from last year's FCF and the chosen terminal growth converts that into a terminal value via Gordon growth and sums the present values to produce enterprise value and equity value. Uh so enterprise value minus debt. Uh so this would be EV minus net. Uh the built-in quality control check is something else that we have. So the function raises an error if terminal growth is greater than WHCC and this prevents aggressive terminal assumptions from silently driving valuation because strategic biders may pay more. The synergy decomposition happens in the compute synergy components and apply synergies decomposed. So the compute synergy components simply reads the synergy patterns and applies synergy decomposed constructs a proform model that layers three revenue effects digital uplift incremental new store revenue and store overlap since cannibalization/cannibalization and an EBIT TDA uplift um from operating leverage. The code applies a yearby-year capture schedule so synergy realization is realistic and ramped and the function returns a pro-forma FCFS which we then safely substitute into the GCF.\n\nSo for marketbased checks which is the next one we have a comp precedent valuation which computes pure set mean/median EV/ EBIT date TDA and P/E multiples and applies them both applies both the president EV divided by EBITDA band multiples to the year five metrics to produce comparison bands. This triangulates the DCF range with what the market has historically paid for similar businesses. To explore sensitivity, the R script includes sensitivity walk terminal. It sweeps uh WACC and terminal growth. And we also have sensitivity synergy capture. This shows how strategic uh EV moves with percent of synergies captured. Uh and we have a table which I will talk about later which can kind of demonstrate a few of these things in our output. Visual output is handled by a few small plotting helpers. We have plot EV versus WACCC. It draws EV lines for different terminal growth assumptions. Plot synergy captures and shows how strategic EV versus capture percent and football field plot. Uh draws the final football field synthesizing DCF bands, precedent bands, and strategic u projections. When you call uh our full run valuation here uh the script prints the projection checkpoint, the DCF summary uh the comp value synergy proforma EV and the sensitivity tables and finally the football field ranges. Um\n\nso now our script concludes with a\n\num as we can see here. But our script concludes with an executive recommendation block uh as we see here that deres a numeric walkway opening bid a negotiation target from the base DCF and precedent bands and a print rubric coverage helper that maps the files outputs to the assignment uh criteria here uh we can see here. So it's really easy to rerun the model with updated assumptions uh adjust store opening schedule or t tweak synergy capture and change immediately through flows uh through all outputs. So it's a really kind of editable and um is not just situation specific. So in the validate stage, we pressure test the model's logic uh the assumptions driving each scenario and the reasonleness of the outputs. This is where we confirm that the math aligns with the finance intuition and the results behave in a way a real deal would. So now here in our validate stage, we're basically going to be seeing our output and making sure that uh it makes sense. So we start by checking the directional accuracy across all three scenarios. All three cases produce positive free cash flow from year one. uh which is consistent with a mature 2100 store operator revenue and EBITDA growth uh also scale appropriately. Uh the base case moves from 6.4 billion to 8.2 billion and the upside from 6.7 billion to 9.0 billion uh rounded and even the downside grows modestly. These trajectories confirm the model is com compounding correctly rather than spiking or collapsing unexpectedly. Next, we evaluate the DCF structure. Each scenario has a terminal value that dominates enterprise value uh ranging from about 68% in the downside to 78 uh% in the upside. and\n\nwhich is normal for a multi-year retail forecast with long run cash generation. The discounting math is consistent as WACCC increases EV falls. As terminal growth increases, EV rises. Um as we see here uh the sensitivity tables uh here reflect uh smooth monotonic changes confirming that the DCF engine is functioning properly properly. Um\n\nfor the upside case as we see here we additionally evaluate synergy logic the synergy components so store build up digital lift and operating leverage roll into a strategic EV of 14.8 8 bill 8 billion and the sensitivity table here uh shows a controlled setup in value as capture increases the relationship is linear and consistent which indicates that synergy value is being added rather than double counted.\n\nsee all here with their sensitivity table.\n\nFinally, uh we see here that our football field outputs show clean separation between downside protection, base triangulation and upside optionality. The walkway price aligns with the low end of the base DCF and the negotiation target lines up with the midpoint of the overlapping ranges. These relationships confirm that the recommendations anchored in the model rather than arbitrarily chosen. And we see here as our very very final thing, we have our executive recommendation.\n\nSo we have our walk away price, our opening bid, negotiation target, strategic buyer services, all here as part of our executive recommendation. all of which come from all of our values here.\n\nSo now that we've gone through the code and talked about it a little bit um and you can see some of the outputs here that it's also produced um we're going to go ahead and move to the evolve stage of our driver framework. So in the evolve stage, we lift the analysis beyond the baseline requirements and explore how a sophisticated buyer or a more advanced modeling framework would build the foundation we've created. This is where we stress test the investment thesis, incorporate dynamic levers, and address evaluation angles that weren't explicitly required but meaningfully enhance the credibility of our work. One natural extension is to look at the risk adjusted returns rather than value alone. Um\n\nuh because the DCF already produces consistent cash flow patterns across scenarios, we could upgrade the model to full probabilistic valuation via Monte Carlo simulation. Instead of a single valuation per case, this would produce a distribution of outcomes driven by randomized revenue growth, uh margin expansion and discount rates. The benefit is that we would not only know the expected enterprise value but also the probability that the value falls below the walkway threshold. An insight critical for private equity decision-m. Another enhancement is integrating a lightweight LBO structure to estimate investor IRR. Although this isn't central to our assignment here, it would allow us to validate whether the acquisition remains attractive after layering and leverage and a realistic exit multiple. For example, stress testing debt service against the downside cash flows would show how much flexibility the deal has before covenants become binding. This turns a valuation exercise into a true deal quality assessment. We can also push deeper on synergies. The upside scenario already incorporates operational synergies for a more advanced perspective when quantify integration cost timeline phasing and a realistic capture rate instead of assuming fullr run rate synergies on day one. We could stage them across three years and calculate the net present value of synergy realization. This refined view would allows would allow a buyer to defend a higher bid by demonstrating not just the magnitude of the value but also the timing and risk attached to it. Um finally from a strategic perspective we can evaluate the deal's broader investment rationale competitive dynamics digital present penetration trends and the long run on new store development. Even modest ships shifts in digital mix or corporate store ownership structure could materially alter valuation profile. Embedding these strategic levers inside the model transforms a static case study into a living tool for scenario experimentation. So now that we've kind of gone uh beyond the assignment parameters and explained some of that, we're now going to move to the final stage which is the reflect stage where I've stepped back from the mechanics of the analysis and consider what this process taught me about valuation decision-m and using analytics to support high stakes judgment. When I began this assignment, the goal was to simply produce a defensible valuation range. But working through the full model, especially across the base, upside and downside scenarios, I highlighted how much the valuation is really about understanding drivers, assumptions, and risks, not just the final numbers. Seeing how small changes in growth margins or discount rates translated into billion dollar swings made the sensitivity of these models real. I also realized how important it is to triangulate rather than rely on a single method. The ZCF alone provided a disciplined cash flow-based view. The trading comps and precedent transactions gave me crucial context for what the market has historically paid for similar businesses. And a synergy adjusted case reinforced what strategic buyers can justify higher bids than financial buyers. Pulling all these perspectives together made the final bid recommendation feel less like a guess and more like a wellsupported position. right here. Uh reflecting on the technical side, collaborating with AI on the heavy modeling work gave me the space to focus on interpretation, checking logic, analyzing outputs, and translating them into a narrative that a real investment committee would understand. This reinforced idea that tools don't replace judgment. Uh they amplify it when used correctly. Ultimately, this valuation exercise taught me more than how to price an acquisition. It taught me how to think like an investor. I had to balance upside opportunity with downside protection, decide when to hold firm and when premium is justified, and communicate the logic behind those decisions clearly. That's what makes a driver framework valuable. Each stage builds not just a model, but a mindset. This has been my session 12 assignment and I'm going to go ahead and run through our code output one more time so you can kind of see some of the outputs. We have lots of sensitivity graphs here. We have DCF summaries uh sensitivity and walk uh WCC and terminal growth. um static EV versus synergy capture here. Percentage um\n\nsome more DCF summaries\nand finally have our executive recommendation here. So this has been my video for the session 12 video assignment. My name is Alex Emenhiser. I've truly enjoyed learning about and working through the driver framework of learning. And while this is the last session, unfortunately, um there are definitely going to be a lot of things that I'm going to be able to take away from this class. And these sessions have not only made me more confident uh with coding and with AI, it's made me more confident as a speaker. And I feel confident that I'm going to be able to take some of these skills that I've learned from these videos and apply them into my future career. Thank you so much for watching. Have a great day."
}