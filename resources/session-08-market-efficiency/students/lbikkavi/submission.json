{
  "student_name": "Lahari Bikkavilli",
  "org_defined_id": "035990933",
  "username": "lbikkavi",
  "email": "",
  "transcript": "Hi, how are you? This is my session eight assignment. So again, as normal on the left, I have my actual like written summary and kind of what I use to understand the assignment as a whole. And on the right, I have the actual code that we'll go through first. So starting up here, we have all of our import statements. Again, these are standard pretty much across any of our assignments nowadays, especially because they're getting a little bit more complex. So we have all of our import statements. And it also says this time like from so where we're importing it from. And it's kind of this little bit more towards like the data and really understanding the data and what goes on with that aspect of it. Moving forward, then the first thing that we have, and I try to always add some sort of comments or something into the codes that when I look back at it later for my future reference, I at least understand a little bit more and I kind of know what I was planning on doing here. So firstly, we have the defined constants in Windows, which is the R stage, right? So then we have the ticker GME, which we're just saying is GME, and then ticker market. And then we have S&Ps at 500 here. Then next, we have our estimation window. So this is the pre -event for market model, which is one year prior to initial rally. And so setting the dates, we have our estimated start date and our estimated end date. So estimated start date is going to be August 1st, 2019, and estimated end date is going to be one year later on August 1st, 2020. Then we have the baseline window. So for volume slash normal price comparison, which is the August to October 2020 range, we again have our baseline start. So baseline start here is going to pick up exactly where the estimated end here is. So August 1st, 2020 as well. And then baseline end is going to be through Halloween, so October 31st. Then our event window, essentially we're just setting our dates so we know what range we're looking at as far as time-wise. Event window is going to be the core squeeze period, so for AR slash car calculations. It's going to be January 1st, 2021 through February 28th, 2021, so basically two months entirely. Then we have August 2020 for precise fact-checking, so August start, which is just going to be the start of August and the end of August. Now here, this is one thing that's a little bit different from some of the code that we've done. where now we're downloading necessary data and stuff that we need for the actual code in general. So here we're downloading all necessary data in one go, which includes the full period. So then an important fix here is that you're forcing auto-adjust, which is false. So we keep adjust close and then high-volume columns. So this is all the data that we're putting in here. As you can see, ticker GME and then ticker market as well. Then again, our start, estimated start, and our event end, which is numbers that we just went over. And the auto-adjust is going to go to false. Then number two, we're going to have our fact verification module. So this is its first stage, which fulfills the D stage verification in that sense. So we're defining the check fact verification, and we're using the data there. Again, this comment here just kind of explains what's going on. So what does this actually mean? It means that we're verifying the professor's claims about GME price in August 2020 and confirm the pre -price specifically. So here it's going to do a regular print statement, which we'll see a little bit more later during my output. But it's going to correctly slice August 2020 data. So August 2020 data equals data from the adjacent close, ticker GME location, .loc, sorry, August start date to August end date. And then our minimum price is going to be whatever the August 2020 data, the minimum of that was, and then the August 2020 data, the maximum that is. So we're essentially taking our data that we've already imported into our code and we're pulling numbers directly from it and we're setting our min price and our max price to use later in our calculations. Here are more print statements where we go over claim verification, stated price in August 2024. Then here, actual GME, and then more print statements in that sense. Next, we have the use dynamic comparison logic. So this is the ratio to claim, which is, again, we're using that min price. And the minimum price is what we pulled from the original data that we're importing into the code. And we're dividing it by that 0.04. Then here, it's going to show what the conclusion of that is. So the conclusion is that the $0.04 claim is incorrect. Actual prices were roughly, and it's going to show what the ratio to claim was, higher than claimed. So this here is just essentially saying that we know that it's incorrect, but it's going to pull in the exact numbers in this ratio to claim, and it's going to format it to one decimal as a float. And so we're going to know exactly what is being pulled in here, and then we're going to know that it was higher than what we originally claimed. Then we have the check January 2021, which is the peak price claim. So January 2021 high is essentially what we're labeling our variable as, which is data high, and then we have the ticker GME. So, again, these are the same dates that we talked about before, so going from January 1st through the end of January, and we're doing the maximum of that, which is why it's the January 2021 high. We're essentially taking our data that we have, we're separating it into this specific ticker GME, and then this specific date range, and then we're pulling the max from that date range to put into this variable. Then here are more print statements, so we can see claim verification stated peak surge was $483 in January 2021. So essentially the max value that we were able to get from this Jan 2021 high is 483. Then we have the actual GME in Jan 2021, which is peak, and then it's going to pull in what that value is, which we'll see later. This is again just a print statement, trying to get formatting a little bit better and nicer there. Then we have the volume analysis, which is the first stage here, which fulfills week from test input. So we're defining the volume analysis using the data, and that's going to essentially calculate and compare volume between the baseline and event periods, addressing the tenfold claim that we talk about later in the assignment so print we have the volume analysis week from test input and just again i do have all of the information that kind of summarizes what the product what the session assignment is about and kind of what i did to get there but i think at least like generically going through um the original code and kind of understanding where that code came from and then we'll get to that aspect after um then we have the volume analysis week from test input volume gme is the data volume and then wrap the ticker gme this calculate basically baseline volume August to October 2020. So baseline volume is equal to the volume GME, baseline start, baseline end, again our time range, and then we're finding the mean of that. So that's why I just calculating the base volume from these dates. So we're calculating it from the August to October 2020, which I'm pretty sure it started from August 1st and then through end of October. Then we're calculating event volume in the same way. So January 2021, the peak month, sorry about that, there's some background noise. The January 2021, which is the peak month, event volume equals volume gme then we have our volume ratio which is event volume over baseline volume and again more print statements here that are going to make a little bit more sense going forward so average daily volume we're using this baseline using our peak month that we just calculated and then the ratio between them so as you can tell this is stuff that we had just calculated and now we're going to kind of output them and see how they compare to each other because that's the whole point right how to compare the numbers so in conclusion here we can see that the volume ratio that we're formatting again to two decimal places increases empirically confirms the retail volume spike, indicating massive trading activity, which we'll explain a little bit further and kind of what that means in reference of it. Then we have the core event study and significance testing. So the run event study data, and then we have calculated daily returns. So our returns equal data, the adjacent close, PCTChange.dropNA, RGME, which is the returns for it, and then returns for the ticker market. Then we have our market model estimation, which is the R stage of implementation and essentially again using the same range of dates so the estimated start estimated n and then drop na for these I decided that a graph would be the best way to understand these which is why you can see that for the model there's the y and x so I'm not going to spend too much time going through the actual code of how it works just because for me personally I feel like I understand the actual graph a little bit better than the code but you can see the code here and it drops any of the missing values safely it's just a little bit easier to see it because I personally think that a graph was the best way to output all the data so we understand it a little better. Then we have our market model estimation and our alpha intercept beta systematic risk model R squared and then again another print statement to make sure that we're printing our lines and things like that that we need. Then we move into our AR calculation our first stage so our GME event and then again we're using the same data range and I think that's kind of the main thing that you see is that all of our calculations for this homework is we're pulling numbers from the data and we're using this to calculate and then making an assumption or proving an assumption wrong based off of that. Then we have our expected event which is going to be the alpha hat plus beta hat times the r market event and then ar event is going to be the rgme event minus the r expected event and this is abnormal returns which is what that ar stands for cumulative abnormal returns which is what that car stands for and then abnormal return results so semi-strong slash weak test total car is going to be january 1st through february 2028 we'll see this a little bit more later and you can see the different variations here again we're formatting the two decimal places we're taking that car event which is that again cumulative abnormal returns Then we have the print statement here, which is the statistical significance test. So this is kind of the entire part where statistics comes in in some sort of way where it's either are we rejecting this hypothesis or are we accepting the hypothesis, or we're not rejecting the hypothesis, I think is the more proper way to say it. Then we have a one-sample t-test, which is testing if the mean AR is significantly different from zero. So we're going to use a t-statistic and a p -value that we already have, and we're going to put this into our stats test. It's going to drop the NENs before test. again null hypothesis and we're going to prove it a little bit more i have a few like diagrams and graphs at the bottom that i think help explain this really really well so we'll keep forward to that so here we know if p value is less than 0.05 so we're assuming the alpha value in this case is 0.05 we're assuming p value less than 0.05 so we reject the null hypothesis this means ar is statistically significant confirming market inefficiency so essentially like we have our hypothesis are we proving or disproving it using these numbers else if we find that the p value is not less than the 0.05 and it's actually greater than we fail to reject null hypothesis and that's kind of what i was talking about a little bit earlier where it's not necessarily we reject and we accept but it's more we reject or we fail to reject and ar is not significantly statistically significant moving forward we have our visualization model all of this honestly the axes etc i can like show you all the code here but most of it is just going to go into what the actual graph making is and this is something that i did multiple prompts with um gemini and i even like tried to help it a little bit because originally just gave me the numbers and honestly had a very very slim output but for me one of the things that I first needed was I know I needed comments so I took the time to put in comments into the code and after I got the comments into the code and I made sure okay if I looked back at this code six months from now would I be able to understand what's going on then I moved into okay what's like a better way to explain and show this data and I think a visual display is obviously always the best way to go so I decided to go with the graph and I prompted AI a little bit on what colors to use and what to make the axes and such which will be a little bit more easy to explain when I kind of go back down. This execution block is basically fact -checking, it's going to verify all the data, then we're doing the volume analysis, then there's a core event and significance test, this is the accept or reject, then the visualization which is the all of the actual charts and stuff we have, as you can see it's a little preview of some of them, and then we have the print statements where it's going to show print statements. So now I'm actually going to take a minute and kind of switch over to the other aspect which is the overview. So this is kind of, the left side is honestly what I do consistently just because I think that it's something that I need to understand the code and understand the assignment as a whole so overview the assignment analyzes how much return investors require from three different companies based on risk levels so we're using their um oh I'm so sorry this is the wrong I'm so sorry about that this is the wrong one I think this is my old one that kind of pulled into there so um if you give me one minute I'll pull up the right one real quickly okay sorry about that I think I accidentally copied and pasted the wrong one here so moving forward a little bit essentially what each part of the code does right so an explanation and that's kind of what I put into it like a friendly explanation of this so originally we're gonna set up our imports and our time windows which is what we went over at the beginning so we have our estimation window essentially what each of these estimation windows really mean is used to calculate for example the first one is what normal GMU returns should look like within that range then we have our baseline volume window which is going to be used to measure used to measure normal trading volumes and used to calculate what normal gme returns should look like then the event window is going to say the short squeeze period where you examine the abnormal rates august 2020 window is going to be for fact checking just the 0.04 claims remember we saw earlier the four percent or not four percent sorry excuse me four cent claim and kind of how we can approve and disprove that so the august 2020 window that data was pulled specifically to prove or disprove that specifically next we downloaded the data this is the part that i thought was honestly really different from any of their homeworks or session assignments that we've done, which I really enjoyed, was that the data was a lot more, it was like we had actual data that we were pulling from and not just made up numbers, which I really appreciated. And so this is just a little bit about the data downloading. Then we go into fact verification. So we checked fact verification. It does two essential things. One, we're verifying August 2020 price. So they're doing GME's price between August 1st to 31st of 2020, and we're calculating the real min and max price. So this is what we see a little bit earlier in the code, or we're actually calculating the price. Then we're taking that calculator price and we're complaining we're comparing them to the claim that four cents in august 2020 is kind of what the norm was and the result of this that we did see a little bit earlier which you can see down here as well um should be here so stated price 0.04 in august 2020 actual gme this and we can see this 0.04 claim is incorrect actual prices were 25.9 times higher than claimed and that's kind of what you can see here so um actual prices around four dollars not four percent so maybe here it says 25.9 but here it says around 100 times i think the 25.9 is a little bit more accurate this is more math that i kind of did a little bit on myself and tried to put it through and kind of tried to get some ai to help me as well so this verify and then the next thing that we move on to is verifying january 2021 and that intraday peak so we use the high column to find the exact peak price in jan in january and it concerns that confirms that 483 dollars is the preak so that right here is we're going to see the claim verification it's going to say stated peak surge let me see if i can make this a little bit bigger here just so it's a bit easier to see. We're going to see the claim verification state of peace search 483 in January 2021. So essentially everything that's on the left is going to correspond with everything that's on the right. I just personally think the information on the left is a little bit more flushed out and I do realize that the flushed out version is not something that you can really have as far as like in the input for the code just because you do want to focus on keeping it a little bit cleaner on that end. So I normally tend to try and make a little bit more of a me version of the information on the left hand side here. And then this satisfies the critical first step of my assignment that's kind of what was going on volume analysis is the weak form price in the volume test so we're using the volume analysis function which we saw which calculates the baseline average volume event average volume and the ratio and we have the different formulas here which we went over already so I won't bore you with going over them again and it's kind of where we are right here volume analysis so it shows you like the baseline that we use for each of these so average daily volume average daily volume and how many shares came as a result of that so if the ratio is close to 10 times this confirms the claim that retail trading exploded during the squeeze. This provides empirical evidence from weak form market activity price and volume patterns. So we can see here there's a volume increase ratio from the January 2021 baseline, which is 5.81. So we can see that there was an increase here. And the conclusion is that the 5.81 increase empirically confirms the retail volume spike, indicating massive trading activity. So because we can see that there is such a difference between the baseline from August, October to January 2021, we know that there is a spike here and we do see the massive trading activity difference here which proves what we were originally thinking moving on to market model and abnormal result abnormal returns sorry about that i just took an exam so i think i'm a little bit all over the place but um the function is the run event studies so this is the most important analytical section of the kind of assignment in general where you're estimating the normal gme return and then we have our estimated this is exactly pulled directly from the code it's something i just copied and pasted over but it gives a little more insight so the regression gives the alpha which is the intercept the beta which is gme sensitivity to the market and then the R-squared, which is the fit of the model. This is the baseline of the expected return model, which I'll show a little bit later. So again, these are more calculations, which we've already seen and kind of going over a little bit in the code. But going over here, we can see the total car, which as we discussed, for this range specifically, 3.8879, which is IE 388.79%. So the max daily AR is going to be 1.3754. And we're going to see it shows the total buildup of abnormal performance. This directly tests semi -strong efficiency. Moving on, we have our t-test for EMH violation. So this is the same thing that we said because we know that the p -value is less than the 0.05 claim, which is going to be statistical significance. It's going to show it right here, the validate stage, right? Like, how can we validate this hypothesis? So our original null hypothesis, which is, in this case, the way statistics works, is what we're assuming is going to be true, is that the mean AR in event window equals 0, right? Market is efficient. That's what we're assuming. Now, mean AR, we have 0.102313. T statistic that we got is 1.63, but the real value that we want to look at is 0 .116. So the conclusion is that 0.116 is greater than 0.05, so we fail to reject null. AR is not statistically significant, right? And so for this, I kind of had a few different visualizations and stuff that I thought would make it a little bit easier. But before that, I did want to go through and kind of showcase what my driver cheat sheet was and where I use driver specifically in the assignment. So for D, Define and Discover, I first identify like the core research question, which is, did the GME event violate market efficiency? And that's kind of the big thing that we're focusing on. I probably should have started with that, but that's kind of what the assignment focuses on. And this is the needed information. So we need historical prices, volume, market proxy, and an event timeline, right? We need the historical prices just so we know what we're comparing to. We obviously need an event timeline so we know the range of time for our data that we have. Key claims that we need to verify is the $0.04 price in August 2020, the $483 peak price, and that 10 times retail volume spike. So R for represent is going to say, you design how the analysis will work. This is a market model regression to compute the expected returns, event study windows, estimation windows. This is where I use AR, CAR, and T-test to evaluate that EMH. This is the game plan stage. It's kind of the way that I read it when I looked online a little bit more. This is where I'm actually starting to understand, okay, this is what I'm planning on doing, and this is how I'm going to do it. Implement. This is my code. This is me implementing my Python. This is me going to Gemini and prompting it over and over again and trying to get the code that I want and the visualizations that I personally want. And so moving forward, then, that's, like, the downloading the data. This is me fact -checking, like, the claims, things like that, computing AR, running a significant sense, and finally generating the plots that are at the bottom here. Next, we have validate. This is where we do the, we decide what the results mean, right? We're interpreting any of the results that we got. So we're checking if it's statistically significant, which is this entire last section here, right? Are we rejecting the null? Are we failing to reject the null? In that sense, all of those things. Then E involves what does this mean for active versus passive investing? What does this teach about risk and short selling? Things more like that. How can we use this to discuss implications going forward? Finally, we have reflect, which is to tie the EMH and behavioral finance together. This is overconfidence, things like limits to arbitrage, things like that. So reflect on what GME events reveal about real world markets. Now getting into a little bit more of the more important stuff, in my opinion, is understanding these visualizations. So I'm going to full screen just to show the visualization. So our first graph is going to be the GME price behavior and key events from the August 20th to 2021. So I have a little key here that I did have Gemini put in just because I thought personally it was really confusing. The blue line is going to do our GME price that adjusted close. So we can see there's a spike here that's a little bit later. So a little bit later, we can assume it's maybe closer to the end of that February 2021 period, maybe a little bit into 2021 there. Then we have the stake for the September 20th, which is going to be this green line here. Then the squeeze peak trading restrictions that we have. So going into my analysis again a little bit what it is, the graph shows how GameStop's price moved leading into and out of the short squeeze. So the graph shows the blue line, the GME adjusted closing price over time. The green dashed line is that 13% stake that Ryan Cohen bought. this is the first major catalyst the market started believing gme could turn around red vertical is going to be january 2027 this is sorry january 27th 2021 the peak of the squeeze right so this is where everything kind of took its like had it was at its highest point this is retail demanded exploded while short interest was still greater than 120 this is the part that i really want to focus on the why it matters why does this visualization matter it visually proves no offense Mr. No offense, Professor Zhang, but it proves that your claim was wrong. In August 2020, GME was around 3 to 5, not 0.04. And that's something that we can see here a little bit with the prices on the Y axis there. So we can see that this is not clear. You can clearly see the climb into the squeeze and the explosive peak at that 483. So not only are we proving it with numbers, but now you can physically see it with your eyes using the data that we imported. Next, we have the GME trading volume, which is on the right here. So trading volume is going to have, again, a little bit of a key thing we're going to see that we have volume millions of shares on the left side and that's kind of what our graph looks like and so this bar chart shows how many shares were traded each day from scaled into millions just to make it a little bit more readable the graph shows baseline trading which is 5 to 30 million shares a day and then the peak squeeze which is this area right here it's going to show the 500 to 800 million shares a day again why does this matter that confirms the statement that retail trading volume increased 10 times or more we can see it increase taking this peak here for example i hope you can see my cursor compared to something over here we can see that 10x increase a little more. The spikes align perfectly with the price explosion which shows clear herding behavior and extreme market attention there. Finally the last graph here is going to be the cumulative absorption returns which is that car during event window of the January 1st of February 2021 and the line is going to show the actual car value and so the graph shows how much GME's return deviated from what is predicted to return and it shows the starts near zero no abnormal movement and it spikes to over 400 percent. The abnormal returns during the squeeze so why this matters ultimately the car plot provides direct evidence of emh so we see that it's large sustained statistically significant abnormal returns which violate strong market efficiency market prices did not instantly adjust to this information but they were driven by behavioral forces coordination and constraints basically short covering and trading restrictions so how the graphs fit together we see that for the price chart we see price jumps far beyond fundamentals volume massive retail buying frenzy where everyone wanted to buy everything at that point which is because the high shares per day and then the car chart is the huge abnormal returns so we can see that this is semi-strong form likely violated hurting things like that and then market model cannot explain the event together it shows that there's a coordinated social media driven bubble inefficiency and short-term price formation and there's a clear like behavioral finance mechanism here like hurting overconfidence we can see that the market moves collectively together and then we see limister the arbitrage here which is the hedge fund short squeeze prevented rational correction so overall this is my assignment I hope it made sense to you but this is a little bit way of how I decided to take it on so you can see I have my general output and then I have the graphs that honestly help me understand the information a little bit more but yeah that's it thank you",
  "video_url": "https://youtu.be/yxU0WwOoKXE"
}