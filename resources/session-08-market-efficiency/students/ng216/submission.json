{
  "student_name": "Justin Ng",
  "org_defined_id": "038445183",
  "username": "ng216",
  "email": "",
  "transcript": "0:00:00.480,0:00:05.280 Hey guys, welcome back. It's Justin here. So today, let's move on to the market efficiency 0:00:05.280,0:00:12.960 investigation. So today I'm going to talk about the GME which is GameStop company. 0:00:12.960,0:00:17.360 So the purpose of this project is to use the GameStop short squeeze as a real world test 0:00:17.360,0:00:22.880 of market efficiency. Instead of just relying on theory, right? I want to see how GME actual price 0:00:22.880,0:00:28.560 behavior lines up with the different forms of the efficient market hypothesis and how much of the 0:00:28.560,0:00:33.600 story is better explained by behavioral finance and limits to arbitrage. So in simple terms, 0:00:33.600,0:00:40.160 did the market act efficiently during GME or did it break in predictable ways? To answer that, 0:00:40.160,0:00:44.400 I first need a clean accurate picture of what actually happened. That's where discover and 0:00:44.400,0:00:49.600 defiance comes in. So in the discover and define stage, I start by making sure the basic story and 0:00:49.600,0:00:58.080 data for GMBB are correct. One key example is the claim that GME traded at $0.04 in August 2020. 0:00:58.080,0:01:04.720 When I check split adjusted price, I found that GM was around $1 to $2 that month with a closing 0:01:04.720,0:01:13.440 price of around $1.67 on August 31st. So that the 0.01 number is simply wrong. And that's why I'm 0:01:13.440,0:01:18.960 building everything on verified data instead of just copying the statements. So to organize the 0:01:18.960,0:01:26.720 event, I create a simple timeline in Python. You can see the table here. So first um I define a 0:01:26.720,0:01:32.640 list called events. Each row on this list has three different uh pieces which is the date, 0:01:32.640,0:01:38.400 a short event label and a brief description about it. So for example, I have one row of 0:01:38.400,0:01:44.640 order August 31st. You can see here which is the price check and another one for the RC ventures 0:01:44.640,0:01:52.160 this one. So and another one for December state increases. So I have basically everything here 0:01:52.160,0:01:58.320 and this is the table of it. I convert that list into a data frame with this line you can see here. 0:01:58.320,0:02:06.320 This is very important and that's how I got this table because um now I can easily sort print and 0:02:06.320,0:02:11.760 refer back to it and when I display timeline BFF the output is a clean table with one row per key 0:02:11.760,0:02:18.000 event. That table is basically my master timeline. It shows when khan increased his states when he 0:02:18.000,0:02:24.560 joined the board when the squeeze pack when the squeeze p and when trading was restricted. So I'll 0:02:24.560,0:02:30.560 use those data later to define at event windows and line up my abnormal return calculation. So 0:02:30.560,0:02:37.440 let's move on to the represent part. So in the represent stage I focus on the design of the 0:02:37.440,0:02:42.560 analysis rather than on actual calculation. Here I'm answering the question which is how exactly 0:02:42.560,0:02:48.720 am I going to test the market efficiency using the GME timeline. So the first thing I do is create a 0:02:48.720,0:02:54.240 table that links each form of the efficient market hypothesis to how I handle it in this project. You 0:02:54.240,0:03:00.960 can see the I build a data frame with these three columns EM form the short definition and how it is 0:03:00.960,0:03:07.920 addressed. So for example the first one which is the wick form it says that the prices reflect all 0:03:07.920,0:03:13.520 past price and volume information and I note that I'll look for momentum and reversals in abnormal 0:03:13.520,0:03:20.560 returns around the squeeze and the semi strong one is says that the prices should reflect all public 0:03:20.560,0:03:25.600 information and I explained that I'll use an event study of cumulative and normal returns around 0:03:25.600,0:03:31.840 public news like the 13D feelings around here the board use the squeeze and hearing and the last 0:03:31.840,0:03:39.120 one which is a strong form efficiency row two but there I note that it will be handled qualitatively 0:03:39.120,0:03:46.240 by discussing who gain and who lost this one. So let's move on to the second table which is I 0:03:46.240,0:03:51.680 create this table that defines my different event windows. This table has three columns which is 0:03:51.680,0:03:57.920 window type the range and purpose. For example, the estimation window is -250 to -30 trading days 0:03:57.920,0:04:04.720 before event for the estimation window and it's estimation and is purpose to estimate CAPM alpha 0:04:04.720,0:04:11.120 and beta. The short event window is negative -1 to + one days around the event which I use 0:04:11.120,0:04:16.720 to measure the immediate reaction and the medium window which is the second one. You can see here 0:04:16.720,0:04:24.400 it is a second but it's actually the third one. So which is uh it's around negative five to five 0:04:24.400,0:04:31.040 plus 5 days which lets me see short-term drift or overreaction. And the last one which is the 0:04:31.040,0:04:37.360 squeeze aftermath window which is the plus one to plus 10 trading days after squeeze speed. It 0:04:37.360,0:04:43.040 helps me to capture the reversal after the bubble pops. And the last one, I also build a table that 0:04:43.040,0:04:49.120 lists the specific events I'm studying, which is one row for initial RC venture state disclosure, 0:04:49.120,0:04:56.000 one for the state increases, and one for the bot announcement. And the fourth one is a short speed 0:04:56.000,0:05:02.960 and trading restriction. And the last one is a US house hearing on GME and the market structure. 0:05:02.960,0:05:10.480 So each row has an ID, a date, and a description. So the important thing about these tables is that 0:05:10.480,0:05:15.680 they encode my design in a very clear way. They don't calculate anything yet but they will show us 0:05:15.680,0:05:24.640 like which state matter which window I'll use and which model I'll apply and how much how each ems 0:05:24.640,0:05:31.360 analysis discipline instead of just random that keeps the analysis discipline instead of 0:05:31.360,0:05:37.280 random. So now let's move on to the implement stage. So in this implement stage I take the 0:05:37.280,0:05:42.320 plan from represent and actually turn it into a working pipeline in Python. I split 0:05:42.320,0:05:48.080 it into four different parts but I mainly talk about the first three part first. So the first 0:05:48.080,0:05:52.720 uh table is the data preparation, the second one is the model estimation and the third one is the 0:05:52.720,0:05:58.480 event window calculation. So let me show the first one which is this one. So the first table describe 0:05:58.480,0:06:04.000 the data preparation code I'm using. In a notebook I start from one combined data frame called data. 0:06:04.000,0:06:09.600 It already has the columns I need which is the date, the JME price, the market index price and 0:06:09.600,0:06:15.200 also the last one the risk free rate which is RF. You can see here this line and the step two which 0:06:15.200,0:06:22.960 is sort and clean dates matches the code where I convert the date column to date time and reset the 0:06:22.960,0:06:29.760 index. This makes sure the time series is in the correct order. So that later when I talk about t0 0:06:29.760,0:06:37.840 uh to 250 or t0 to 5 those really are 250 days before or 5 days before the data. Step 0:06:37.840,0:06:43.920 three you can see here is the compute daily returns. In this code I use PCT change which 0:06:43.920,0:06:51.920 is this one on GME and market price to create GME and MKT R. So the output of this step is 0:06:51.920,0:06:57.360 two new columns showing the percentage change from one day to the next for GME and for the 0:06:57.360,0:07:05.760 market index too. And the fourth step is compute access returns. Here I create GME access and MKT 0:07:05.760,0:07:11.600 access by subtracting the risk-free rate which is RF from each daily return. This is important 0:07:11.600,0:07:17.520 because later in CAPM I work with returns above the risk-free rate not just raw returns. 0:07:18.080,0:07:24.320 So let's talk about the last step which is the drop missing values. So this thing corresponds 0:07:24.320,0:07:31.040 to the line where I remove rows with nans in the access returns columns. The first observation 0:07:31.040,0:07:38.720 after PCT change is always nan and there cannot be other gaps. So cleaning up this out give me 0:07:38.720,0:07:45.760 a tidy data set that is ready for regression and event window calculation. So from this table one 0:07:45.760,0:07:54.800 the output is a clean data table with GME RAT the MKT and most importantly the GME access and MKT 0:07:54.800,0:08:01.680 access. So dates are solid and NN are removed. This matters because CAPM and event studies work 0:08:01.680,0:08:08.160 on assess returns not just raw prices. A quick sanity check here is that the dates increase one 0:08:08.160,0:08:16.800 row at a time. You can see here and daily access returns look uh plausible not straight missing 0:08:16.800,0:08:22.880 values. So this is how I got the first table. So now let's move on to the table two. So the second 0:08:22.880,0:08:28.880 table summarize the capm estimation part of the code. This is where I estimate what normal look 0:08:28.880,0:08:35.520 like before the event. So you can see here the first step one which is identify the event date 0:08:35.520,0:08:42.960 matches the code where I locate the event date in data and get its index position which I call t0. 0:08:42.960,0:08:49.200 This index is my anchor point for defining all windows around the event. Set two is a defining 0:08:49.200,0:08:56.640 estimation window stage. It's where I chose a pre-events range such as t0us 250 to t0us 30. 0:08:56.640,0:09:03.920 In the code I convert those offsets into index number. This window is far enough away from the 0:09:03.920,0:09:11.920 event so that the event itself does not cont so that the event itself does not contaminate the 0:09:11.920,0:09:19.440 regression. Step three is a subset estimation data. It cor correspond to taking that slice of 0:09:19.440,0:09:27.120 the data frame and keeping only GME access and MT access dropping any remaining missing values. So 0:09:27.120,0:09:34.960 the output is an estimation sample that contains only clean access returns for GME and the market. 0:09:34.960,0:09:42.640 So step four is where I set up my regression. it. I use MKT access as a regressor and add a 0:09:42.640,0:09:50.480 constant term which is GME access is the dependent variable. Step five is the estimation of the CAPM. 0:09:50.480,0:09:57.040 The output gives me an intercept and a slope. The intercept is alpha and the slope is on MKTSS is 0:09:57.040,0:10:03.680 the beta. So these two numbers and describe how GME typically move relative to the market in the 0:10:03.680,0:10:10.400 estimation period. step six which is the last one here which is how to store the parameters. Uh it 0:10:10.400,0:10:15.360 means that I kept those alpha and beta values and use them later to calculate expected returns in 0:10:15.360,0:10:21.840 the event window. So the result of this whole block is a pair of capm parameters that define 0:10:21.840,0:10:29.120 normal behavior for GMBB before the event. So from table two the output is alpha and beta right from 0:10:29.120,0:10:35.920 the pre pre-event regression of GME access and MKT access with a constant. So these two numbers 0:10:35.920,0:10:42.320 define GME normal relationship with the market before the event. So beta should be a sensible 0:10:42.320,0:10:48.960 size for volatile stock and alpha should be small in daily terms. This gives the baseline needed to 0:10:48.960,0:10:55.520 predict what GM's return should have been absent the event. So now let's move on to the third 0:10:55.520,0:11:01.520 table. The third table explains how the code uses those CAPN parameters around the event to compute 0:11:01.520,0:11:06.880 abnormal returns and cumulative abnormal returns. So the first step which is choosing the event 0:11:06.880,0:11:14.320 window matches the part of the code where I define a range like t0 minus 5 to t0 + 5. This tells me 0:11:14.320,0:11:21.520 which days around the event that I want to study. Step two is extracting the event window data. 0:11:21.520,0:11:27.840 It is the slice of data that keeps only those rows and the columns I needed which is include date the 0:11:27.840,0:11:35.120 GME access and the MKT access. So the output here is a smaller data frame just for the event window. 0:11:35.120,0:11:41.440 Step three, I'm computing expected access return using the CAPM parameters from before. So for each 0:11:41.440,0:11:48.480 day in the event window, I applied this formula. You can see here this one. So in the data frame 0:11:48.480,0:11:57.200 this shows up as a new column called exp which is this one the first one uh which represent what GME 0:11:57.200,0:12:03.360 access return should have been if only normal market movement were were at work. So step four 0:12:03.360,0:12:08.960 which is computing abnormal returns which is AR is where I subtract the expected access return from 0:12:08.960,0:12:16.640 the actual one because you can see the formula here cuz you can see the formula here. This one 0:12:16.640,0:12:24.080 uh that gives me a new column AR that shows how much EM over or underperform CAPM on each 0:12:24.080,0:12:30.240 day around the event. Step five is computing cumulative abnormal returns because we are 0:12:30.240,0:12:37.200 comparing between abnormal returns and cumulative abnormal returns. Right? So in this step uh the 0:12:37.200,0:12:44.160 result is a C series that shows the total abnormal impact of the event as we move across the window. 0:12:44.160,0:12:51.200 So we can so let's move on to step six which is assemble output a table just means that after 0:12:51.200,0:13:00.080 these steps I have an event window table with date j access mkt access exp and c there's six 0:13:00.080,0:13:07.040 things there the key output of implement I use it uh in the validate stage to run the c signific uh 0:13:07.040,0:13:14.000 significance test and then in evolve to interpret what this abnormator says about market efficiency 0:13:14.000,0:13:21.920 and the GME episode overall. So from this table the output is the event window table with the 0:13:21.920,0:13:31.120 date GME SS MTSS exps AR and CR across the chosen window which is as for example is T 0 - 5 to T 0:13:31.120,0:13:40.320 0 + 5 sounds confusing but it's actually only 5 days to + 5 days and that's it and AR shows the 0:13:40.320,0:13:48.960 daybyday uh deviation from CAPM and C stacks those uh deviation to show the total abnormal impact of 0:13:48.960,0:13:56.000 the event. So the quick read is check AR on the event day and watch how C builds through 0:13:56.000,0:14:03.840 the window. So taken all together. So taken all together this output finish the build phase which 0:14:03.840,0:14:09.280 is the data is prepared the normal return model is estimated and the abnormal returns are measured 0:14:09.280,0:14:15.440 around the event. So this is exactly what's needed for validation next which is testing whether C is 0:14:15.440,0:14:22.720 stat uh statistically different from zero and for the evolve discussion about what those abnormal 0:14:22.720,0:14:29.200 move mean for market efficiency behavior and risk. Now let's move on to the fourth table which is the 0:14:29.200,0:14:34.640 last one too. So after finding a real abnormal return around the event, the next question is 0:14:34.640,0:14:40.640 did trading activity jump at the same time because prices don't move like that by themselves, right? 0:14:40.640,0:14:45.600 So if attention and hearings matter, we should see a quick search in participation right around 0:14:45.600,0:14:53.120 the event. So I switch from returns to volume and look for a simple spike benefit pattern. So I use 0:14:53.120,0:14:59.360 a table called DF with date and volume. The code cleans the dates, sort everything in time order, 0:14:59.360,0:15:09.840 which is this part um this part or sort everything in this one and um finds the event date. If that 0:15:09.840,0:15:15.280 exact date isn't in the data, it uses the closest trading day. Then it split into the timeline into 0:15:15.280,0:15:20.640 three parts which is pre-event, the event and the postevent which is all three is here. You 0:15:20.640,0:15:29.200 can see here pre-event event and postevent and this one too. So for each part it calculates 0:15:29.200,0:15:35.280 the average and median daily volume. You can see here I slowly go through it. You can see premeian 0:15:35.280,0:15:41.600 uh event mean, post mean and the medium and the turnover. So if a flow column exists, 0:15:41.600,0:15:49.120 it also shows turnover for all three like the pre-event, the event and the postevent. So finally 0:15:49.120,0:15:54.800 it prints a three row table which is pre-event post and a smaller bar chart. So the change is 0:15:54.800,0:16:02.880 easy to see. So if float isn't available it's it just keeps uh so if float isn't available it just 0:16:02.880,0:16:12.720 skips a turnover and still works. So let me show you the whole thing and this is how I got my chart 0:16:12.720,0:16:17.920 and this is the table and this is the chart. So the table shows a clear jump in activity during 0:16:17.920,0:16:24.320 the event window. The average daily volume is about 1.95 million shares before the event and 0:16:24.320,0:16:30.560 jumps to about 10.07 million during the event around like five times higher and then drops 0:16:30.560,0:16:37.440 back to 1.7 97 million after. And the median volume moved the same way which means it's not 0:16:37.440,0:16:44.080 just one or two crazy days. The activities was broadly higher across the whole event window. 0:16:44.080,0:16:49.600 So if turnover is included it rises too means the flow change it hands faster 0:16:49.600,0:16:58.800 while the squeeze was happening. So you can see the whole everything is right here. 0:16:58.800,0:17:03.840 So now let's move on to the validate part. I tested the cumulative abnormal return which 0:17:03.840,0:17:11.040 is c around the event. So in the shop window you can see the first line here. The first line here 0:17:11.040,0:17:23.680 uh the short window -1 to + one the CR is about 23.75%. And with t = to 3.42 and p = to 0212. So 0:17:23.680,0:17:34.720 I reject C equals to zero. I also ran a robustness window which is 2 to + 2. C is around like 29.10% 0:17:34.720,0:17:44.720 with t= to 0 3.05 5 and P equals to 0.0034. Both test says that the move is statistically real. So 0:17:44.720,0:17:51.840 I compare the price jump to public fundamentals. So at the P the stock traded near 483, right? You 0:17:51.840,0:17:59.040 can see here like this part this one. So while analyst targets at the time were roughly around 0:17:59.040,0:18:05.920 $10 to $40. So there wasn't new positive public news to justify the gap. So during the squeeze 0:18:05.920,0:18:12.320 the price decoupled from public information and the the third one which is the wid form 0:18:12.320,0:18:17.920 efficiency says that past information shouldn't predict returns. So around this episode social 0:18:17.920,0:18:24.720 media flow and sentiment line up with short run price move showing some predictability and create 0:18:24.720,0:18:31.440 a tension with weak form efficiency. Let's talk about the last one which is join hypothesis. So 0:18:31.440,0:18:36.720 if event study test EMH and the return model together, so a bit of model error is always 0:18:36.720,0:18:43.440 possible. But given the size of the CS, a small miss in this model is unlikely to as 0:18:43.440,0:18:50.480 a small miss in this model is unlikely to explain everything. So the effect looks genuine, not just 0:18:50.480,0:18:57.200 so the effect looks genuine, not just modeling quirk. So let's move on to the evolve part. 0:18:58.000,0:19:03.040 So in the evolve stage I stop focusing on the code and start asking myself what do this result 0:19:03.040,0:19:11.360 actually means. So in the first table which is this one. Uh the first row is implication 0:19:11.360,0:19:18.320 for active management. The idea here is that big mispricing like GME can happen but trading them 0:19:18.320,0:19:25.280 is very dangerous. So h funds with large leverage position will hit hard when the squeeze happened. 0:19:25.280,0:19:30.880 So for active managers the lesson is learned. So be careful with crowded shots use position 0:19:30.880,0:19:37.760 limits and stress test for short quiz and stress test for short squeeze instead of assuming this 0:19:37.760,0:19:45.280 stock is obviously overvalued. It will fall soon. The second row which is this one I'll show the uh 0:19:45.280,0:19:51.440 output too. So the second row is implication for passive investing. So for index investor 0:19:51.440,0:19:58.880 GE is just a tiny slice of the portfolio. Even with the even with when the stock exploded right 0:19:58.880,0:20:05.280 and then crashed the effect on broad index was very very small. The effect on a broad index was 0:20:05.280,0:20:10.640 very very small. That supports the idea that the low cost passive investing still makes sense for 0:20:10.640,0:20:16.000 most people because trying to time rare dim whim stocks events is hard and very risky. 0:20:16.000,0:20:20.960 Now let's move on to the third row which is the behavioral finance and limits to arbitrage. So 0:20:20.960,0:20:27.200 here I highlight that J was driven by behavior here like the hering the FO the lottery style 0:20:27.200,0:20:33.040 bets and Bman's hands culture on Reddit. So at the same time there were limits to arbitrage 0:20:33.040,0:20:39.440 which is very high short interest expensive borrow margin calls and trading restrictions. 0:20:39.440,0:20:45.200 These frictions made it hard for rational traders to quickly push the price back to fundamentals. 0:20:45.200,0:20:51.360 The fourth row which is the last row is market structure and policy because GME raises a question 0:20:51.360,0:20:57.360 about how brokers manage risk and explain trading halls. How transparent short interest and margin 0:20:57.360,0:21:02.800 rules should be and also how trading ads are designed. Message is that we may need a better 0:21:02.800,0:21:08.000 balance between easy access for return investors and protection that reduce confusion and extreme 0:21:08.000,0:21:17.280 risk takingaking. So let's move on to the second table. So the second table goes deeper into 0:21:17.280,0:21:23.280 rational investor profitability and leverage. So the first row here you can see that it's X and D 0:21:23.280,0:21:29.520 and X post profit. Looking back it's easy to say that I could b have bought uh like Tesla at that 0:21:29.520,0:21:36.240 price and so at that price. But at the time those trades were extremely risky and hard to time. But 0:21:36.240,0:21:42.320 at that time those trades were extremely risky and hard to time. So there was no simple safe rule a 0:21:42.320,0:21:48.880 rational investor could follow in real time. And row is a systematic profit opportunities. 0:21:48.880,0:21:55.600 It makes the same point which is to really break EMH you want a repeatable strategy in GME. Most 0:21:55.600,0:22:00.000 success stories depend on nearly perfect timing which is not something you can reliable repeat 0:22:00.000,0:22:07.520 on. The third one which is re and the third one which is the risk adjusted returns. Uh it reminds 0:22:07.520,0:22:13.840 us that even if some people made big gains, the downside risk was huge too because like squeeze 0:22:13.840,0:22:19.200 uh because like squeezes the margin calls and the chance of blowing up like some hedge funds 0:22:19.200,0:22:25.760 it the fourth one. Wait before that let me show you the output for it first which is I was saying 0:22:25.760,0:22:30.640 the whole time but I didn't show you at all because basically everything is right here. 0:22:30.640,0:22:36.160 So the fourth one which is the information advantage it knows that return traders didn't 0:22:36.160,0:22:43.280 have private information their age their edge their edge was mostly speed and coordination on 0:22:43.280,0:22:49.520 social media so hedge funds had better fundamental research but were exposed to crowd race so it's 0:22:49.520,0:22:55.680 more about different types of information so it's more about different types of information that 0:22:55.680,0:23:01.520 one side being smart and other so it's more about different types of information that one side being 0:23:01.520,0:23:08.400 the smart guy and the other one is being the dumb guy. Finally is the asset leverage which connects 0:23:08.400,0:23:14.560 to leverage and risk shifting. Highly leveraged funds shorting GME could end up taking even more 0:23:14.560,0:23:21.040 risk after losses. So effectively shifting risk onto leaders and prime brokers. So that 0:23:21.040,0:23:27.360 leverage helped turn a mispriced stock into a full-blown short squeeze event. So overall, 0:23:27.360,0:23:33.360 evolve shows that what the numbers means in real life, such as how active and passive investors 0:23:33.360,0:23:39.040 should think about stocks like GME, how behavior and friction matters, and how leverage and market 0:23:39.040,0:23:46.400 design can make this event much more extreme. So now let's move on to the reflection part on market 0:23:46.400,0:23:52.000 efficiency. J pushed me to a more balanced view. In the short run, the squeeze clearly did not 0:23:52.000,0:23:58.240 look efficient, right? Because prices exploded to levels far above any reasonable fundamental 0:23:58.240,0:24:03.680 estimate, returns show strong momentum and a brief crash and social media sentiment play a 0:24:03.680,0:24:10.400 huge role too. Over a longer horizon, the price moved back closer to the fundamental ranges and 0:24:10.400,0:24:16.400 there still wasn't a simple lowrisk trading rule that would have guaranteed profits ahead of time. 0:24:16.400,0:24:22.960 So I end up seeing ENH as a useful baseline that can break down in extreme stocks like uh 0:24:22.960,0:24:28.560 GME rather than something that's always right or always wrong. I actually also learned a lot about 0:24:28.560,0:24:36.720 the process of doing empirical finance. I had to fix basic facts like the wrong 0.04 04 price, view 0:24:36.720,0:24:43.120 a clean timeline, choose windows, estimate CAPM, compute AR and C and then actually test whether 0:24:43.120,0:24:50.080 C was statistically different from zero. Made me much more aware that good stories need data and a 0:24:50.080,0:24:56.160 proper design behind them. So in the end, I really want to say the driver method really helped me a 0:24:56.160,0:25:01.520 lot. Like the discover and define part made me clean up the facts and state the main question. 0:25:01.520,0:25:06.960 The represent part forced me to design the event study clearly. The implement and validate part 0:25:06.960,0:25:12.560 made me turn that design into numbers and check if they were robust. And the evolve part made 0:25:12.560,0:25:18.720 me stand back and ask these matters for active and passive investing for behavioral finance and 0:25:18.720,0:25:25.200 for how I think about EMH going forward. And that's all I have today. Thank you very much.",
  "video_url": "https://youtu.be/5F-yY5OSf64"
}